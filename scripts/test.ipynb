{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, uuid, sys\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "from azure.core._match_conditions import MatchConditions\n",
    "from azure.storage.filedatalake._models import ContentSettings\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection String to Datalake \n",
    "connectionstring=\"DefaultEndpointsProtocol=https;AccountName=datalaketest123;AccountKey=mG5IGAwpT+tjzf7XQ7TDKqsrpN2ESnMG/CDjPUjSIcUn46mKKlF67b1mRBXDq/J59NuNRImYw/qgCUuTFFX2dg==;EndpointSuffix=core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n"
     ]
    }
   ],
   "source": [
    "# The datalake service client is the class that interacts on account level with the datalake: Goal list the different filesystems/blobs\n",
    "service_client = DataLakeServiceClient.from_connection_string(connectionstring)\n",
    "file_systems = service_client.list_file_systems()\n",
    "\n",
    "file_systems_list =[]\n",
    "for file_system in file_systems:\n",
    "    file_systems_list.append(file_system.name)\n",
    "print(file_systems_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d365datalakeexport\n",
      "\n",
      "d365datalakeexport/ChangeFeed\n",
      "\n",
      "d365datalakeexport/Tables\n",
      "\n",
      "d365datalakeexport/Tables/Common\n",
      "\n",
      "d365datalakeexport/Tables/Common/Customer\n",
      "\n",
      "d365datalakeexport/Tables/Common/Customer/Main\n",
      "\n",
      "d365datalakeexport/Tables/Common/Customer/Main/CustTable\n",
      "\n",
      "d365datalakeexport/Tables/Common/Customer/Main/CustTable/index.json\n",
      "\n",
      "d365datalakeexport/Tables/Common/Customer/Main/CustTable.cdm.json\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/ProductInformationManagement\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/ProductInformationManagement/Main\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/ProductInformationManagement/Main/EcoResProduct\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/ProductInformationManagement/Main/EcoResProduct/ECORESPRODUCT_00001.csv\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/ProductInformationManagement/Main/EcoResProduct/index.json\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/ProductInformationManagement/Main/EcoResProduct.cdm.json\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/SalesAndMarketing\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/SalesAndMarketing/WorkSheetHeader\n",
      "\n",
      "d365datalakeexport/Tables/SupplyChain/SalesAndMarketing/WorkSheetHeader/SalesTable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The FileSystemClient class interacts with a specific filesystem in the datalake: Goal get the pathnames of our desired filesystem\n",
    "for file_system in file_systems_list:\n",
    "    paths = FileSystemClient.from_connection_string(connectionstring, file_system_name=file_system).get_paths()\n",
    "    for path in paths:\n",
    "        print(path.name + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Table</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d365datalakeexport/Tables/Common/Customer/Main...</td>\n",
       "      <td>CUSTTABLE_00001.csv</td>\n",
       "      <td>custtable</td>\n",
       "      <td>csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d365datalakeexport/Tables/Common/Customer/Main...</td>\n",
       "      <td>CustTable.cdm.json</td>\n",
       "      <td>custtable</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d365datalakeexport/Tables/SupplyChain/ProductI...</td>\n",
       "      <td>ECORESPRODUCT_00001.csv</td>\n",
       "      <td>ecoresproduct</td>\n",
       "      <td>csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d365datalakeexport/Tables/SupplyChain/ProductI...</td>\n",
       "      <td>EcoResProduct.cdm.json</td>\n",
       "      <td>ecoresproduct</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path                 Filename  \\\n",
       "0  d365datalakeexport/Tables/Common/Customer/Main...      CUSTTABLE_00001.csv   \n",
       "1  d365datalakeexport/Tables/Common/Customer/Main...       CustTable.cdm.json   \n",
       "2  d365datalakeexport/Tables/SupplyChain/ProductI...  ECORESPRODUCT_00001.csv   \n",
       "3  d365datalakeexport/Tables/SupplyChain/ProductI...   EcoResProduct.cdm.json   \n",
       "\n",
       "           Table  Type  \n",
       "0      custtable   csv  \n",
       "1      custtable  json  \n",
       "2  ecoresproduct   csv  \n",
       "3  ecoresproduct  json  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with the desired files. The csv files contain the data, the cdm.json files the metadata\n",
    "import re\n",
    "pattern = re.compile('\\w.*\\.cdm.json|\\w.*\\.csv')\n",
    "results = [(path.name, \n",
    "path.name.split('/')[-1],\n",
    "path.name.split('/')[-1].split('.')[0].split('_')[0].lower(),\n",
    "path.name.split('/')[-1].split('.')[-1]) for path in FileSystemClient.from_connection_string(connectionstring, file_system_name=file_system).get_paths() \\\n",
    "    if pattern.match(path.name)]\n",
    "# pattern = re.compile('\\w.*\\.cdm.json|\\w.*\\.csv')\n",
    "# new_results = list(filter(pattern.match,results))\n",
    "pd.DataFrame(results, columns=['Path','Filename', 'Table', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SysRowId                                int64\n",
      "LSN                                      int64\n",
      "LastProcessedChange_DateTime    datetime64[ns]\n",
      "DataLakeModified_DateTime       datetime64[ns]\n",
      "RECID                                    int64\n",
      "                                     ...      \n",
      "BLOCKFLOORLIMITUSEINCHANNEL              int64\n",
      "CREATEDDATETIME                 datetime64[ns]\n",
      "MODIFIEDDATETIME                datetime64[ns]\n",
      "DATAAREAID                              object\n",
      "ROWVERSION                              object\n",
      "Length: 71, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read file into a dataframe\n",
    "from io import StringIO\n",
    "import json\n",
    "adl = FileSystemClient.from_connection_string(connectionstring, file_system_name=file_system)\n",
    "metadata_file =json.loads(adl.get_file_client(\"d365datalakeexport/Tables/Common/Customer/Main/CustTable.cdm.json\").download_file().readall())\n",
    "datecolumns = [item['name'] for item in metadata_file['definitions'][0]['hasAttributes'] if item['dataFormat']=='DateTime']\n",
    "columns_names = [item['name'] for item in metadata_file['definitions'][0]['hasAttributes']]\n",
    "\n",
    "input_file = adl.get_file_client(\"d365datalakeexport/Tables/Common/Customer/Main/CustTable/CUSTTABLE_00001.csv\")\n",
    "\n",
    "customers = pd.read_csv(StringIO(re.sub('\"\"\"', '\"', str(input_file.download_file().readall(),'utf-8'))),sep=',', names=columns_names, parse_dates=datecolumns,infer_datetime_format=True)\n",
    "print(customers.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to sql table\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "target_engine = create_engine('mssql+pyodbc://sa:yourStrong(!)Password@127.0.0.1:1432/SQLTest?driver=ODBC+Driver+17+for+SQL+Server', \n",
    "                       echo = False)\n",
    "customers.to_sql(\"customers\", target_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': datetime.datetime(2021, 11, 13, 13, 54, 50, tzinfo=datetime.timezone.utc),\n",
       " 'etag': '\"0x8D9A6AD29ECD145\"',\n",
       " 'last_modified': datetime.datetime(2021, 11, 13, 13, 54, 51, tzinfo=datetime.timezone.utc),\n",
       " 'content_length': 0,\n",
       " 'client_request_id': '45b879e5-4489-11ec-8117-a8a159577461',\n",
       " 'request_id': '7add0b23-c01f-0053-6696-d8aa24000000',\n",
       " 'version': '2020-10-02'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code snippet to add new file to datalake and desired directory\n",
    "file_system_client = service_client.get_file_system_client(file_system=\"test\")\n",
    "directory_client = file_system_client.get_directory_client(\"d365datalakeexport/Tables/Common/Customer/Main/CustTable\")\n",
    "file_client=directory_client.create_file(\"CUSTTABLE_00001.csv\")\n",
    "local_file = open(\"/home/snackinator/Downloads/CUSTTABLE_00001.csv\")\n",
    "file_contents = local_file.read()\n",
    "file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n",
    "file_client.flush_data(len(file_contents))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76bbaa3271aaade04c57d51d2971ff34a756b33b1c3a3dbce0a302f38743fc9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('TotalData-08.11.2021': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
